{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbors"
      ],
      "metadata": {
        "id": "2yj1hswm2Sh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
      ],
      "metadata": {
        "id": "zP_Kks7j2OUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intuition behind the KNN algorithm."
      ],
      "metadata": {
        "id": "_mSgtVVT2hog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-Nearest Neighbors (KNN) algorithm is based on the idea that similar objects tend to be close to each other in a feature space. To classify or predict a value for a new data point, KNN finds the K nearest points to that point in the training set and makes a decision based on the majority of classes (in classification) or the average of values (in regression) of those neighbors. The choice of distance metric and the value of K are important, and KNN is effective for problems where local information is relevant but can be computationally expensive on large datasets."
      ],
      "metadata": {
        "id": "m88M1dzB3WMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm pseudocode"
      ],
      "metadata": {
        "id": "kYDBXuB53Yf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    Function KNN(TrainingSet, QueryPoint, K):\n",
        "    # Create a list to store distances and labels of nearby points\n",
        "    DistanceList = []\n",
        "\n",
        "    # Calculate the distance between QueryPoint and each point in TrainingSet\n",
        "    For each point in TrainingSet:\n",
        "        Distance = CalculateDistance(QueryPoint, point)\n",
        "        DistanceList.append((Distance, point.label))  # Store the distance and label/class of the point\n",
        "    \n",
        "    # Sort the distance list in ascending order\n",
        "    DistanceList.sort_by_distance()\n",
        "\n",
        "    # Take the first K points from the sorted list\n",
        "    NearestNeighbors = DistanceList[:K]\n",
        "\n",
        "    # Count the classes/labels of the K nearest neighbors\n",
        "    ClassCounter = CountClasses(NearestNeighbors)\n",
        "\n",
        "    # Return the most common class/label among the K nearest neighbors\n",
        "    PredictedClass = MostCommonClass(ClassCounter)\n",
        "    \n",
        "    Return PredictedClass"
      ],
      "metadata": {
        "id": "mssolcT78JV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm implementation"
      ],
      "metadata": {
        "id": "uBMo0Pod6NBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset as an example\n",
        "iris = load_iris()\n",
        "X = iris.data  # Characteristics\n",
        "y = iris.target  # Tags/classes\n",
        "\n",
        "# Split the data set into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a KNN classifier with a K value of 3\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Train the classifier with the training data\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"KNN Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq7c3BW_5ZM3",
        "outputId": "36fece08-11d5-4fb2-e773-4d1eed7612a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Iris data set is used as an example data set. The code imports the necessary libraries, splits the data into training and test sets, creates a KNN classifier with a value of K equal to 3, trains it on the training data, and then makes predictions on the test set. Finally, it calculates and displays the accuracy of the KNN classifier on the test set."
      ],
      "metadata": {
        "id": "3W5V7_uZ6QSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function + Optimization function identification.\n"
      ],
      "metadata": {
        "id": "6o789gCn6gSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-Nearest Neighbors (KNN) algorithm does not have a loss function or an optimization function. KNN is a supervised learning algorithm that relies on the idea of finding the K nearest neighbors to a query point in the feature space and making a decision based on the majority of classes among those neighbors (in classification) or the average of values (in regression).\n",
        "\n",
        "Unlike other supervised learning algorithms like Support Vector Machines (SVM), neural networks, or linear regression algorithms, KNN does not involve a loss function that is optimized during training. Instead, KNN stores the entire training dataset and performs calculations in real-time to make decisions based on the proximity of the nearest neighbors to the query point.\n",
        "\n",
        "Therefore, there are no parameters to be tuned through an optimization function in KNN, as is the case with other algorithms that aim to minimize a loss function to find the best model coefficients. KNN is a \"lazy learning\" algorithm in the sense that it doesn't train a model in the traditional sense but rather stores the training data and performs calculations at the time of prediction."
      ],
      "metadata": {
        "id": "-2VaFDwM6iCK"
      }
    }
  ]
}